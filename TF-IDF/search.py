#!/usr/bin/python
import json
import sys
import getopt
import math
import re
from nltk.tokenize import word_tokenize
from operator import mul
from nltk.stem.porter import PorterStemmer


"""
    File name: search.py
    Authors: Svilen Stefanov
    Date created: 12/03/2018
    Date last modified: 24/03/2018
    Python Version: 2.7
"""

term_dict = dict()

# nltk stemmer for stemming of the terms before inserting them in the dictionary
ps = PorterStemmer()

def usage():
    print "usage: " + sys.argv[0] + " -d dictionary-file -p postings-file -q file-of-queries -o output-file-of-results"

dictionary_file = postings_file = file_of_queries = output_file_of_results = None

try:
    opts, args = getopt.getopt(sys.argv[1:], 'd:p:q:o:')
except getopt.GetoptError, err:
    usage()
    sys.exit(2)

for o, a in opts:
    if o == '-d':
        dictionary_file = a
    elif o == '-p':
        postings_file = a
    elif o == '-q':
        file_of_queries = a
    elif o == '-o':
        file_of_output = a
    else:
        assert False, "unhandled option"

if dictionary_file is None or postings_file is None or file_of_queries is None or file_of_output is None :
    usage()
    sys.exit(2)


def get_postings(term):
    """
    #Retrieve the posting list for a given term. If the term is not in the dictionary, return an empty list.
    #:param term: the term which posting list should be returned
    #:return: the posting list for the given term
    """
    value = term_dict[term]
    h = value['h']
    t = value['t']
    with open(postings_file, 'r') as postings_f:
        postings_f.seek(h, 0)
        postings = postings_f.read(t - h)
        postings_list = postings.split()
        postings_list_int = []
        for elem in postings_list:
            elem_list = elem.split('-')
            doc_id = int(elem_list[0])
            tf = int(elem_list[1])
            postings_list_int.append((doc_id, tf))
    return postings_list_int


def compute_norm_doc(query_list):
    """
    Reduce the size of the relevant for the query documents - take into account only documents that contain
    at least 3/4 of the terms that are part of the dictionary.
    Not used for the search because it slows down the search.
    :param query_list: a set of the query terms
    :return: a dictionary of the relevant documents after normalization
    """
    all_docs = list()
    # count the term that are part of the dictionary
    rel_term_count = 0
    for term in query_list:
        if term in term_dict:
            cur_docs = get_postings(term)
            all_docs.append(cur_docs)
            rel_term_count += 1
        else:
            all_docs.append([])
    min_el_count = 3 * rel_term_count / 4
    doc_set = set([item for sublist in all_docs for item in sublist])

    res_list = list()
    if rel_term_count > 1:
        for doc in doc_set:
            el_count = sum(doc in sublist for sublist in all_docs)
            # if at least 3/4 of the query terms are in the doc
            if el_count >= min_el_count:
                res_list.append(doc)
    else:   # query length = 1
        res_list = all_docs[0]

    doc_vects = dict()
    for doc, tf in res_list:
        norm = term_dict['DOC_NORM'][str(doc)]
        val = tf / norm
        if doc in doc_vects:
            doc_vects[doc][term] = val
        else:
            doc_vects[doc] = {term: val}
    return doc_vects


def main():
    """
    Read the query from the query file, reorder the boolean query for faster computation,
    compute the Reverse Polish Notation generated by the Shunting-yard algorithm
    and write the results to the output file.
    :return: None
    """
    global term_dict
    with open(dictionary_file, 'r') as dictionary_f:
        term_dict = json.load(dictionary_f)

    with open(file_of_queries, 'r') as query_file:
        answer_to_queries = list()
        for query in query_file:
            q_vec = list()
            doc_vects = dict()
            stemmed_query = list()
            query_el = word_tokenize(query)
            if query_el:
                N = term_dict['N']
                # Preprocessing: Removing of punctuation and using stemming + lowercase
                for word in query_el:
                    term = re.sub(r'[^a-zA-Z0-9]', '', str(word))
                    term = ps.stem(term.lower())
                    stemmed_query.append(term)
                # remove words appearing more than once because we count them in the t_f computation below
                stemmed_query_set = set(stemmed_query)
                # compute query vector for each term in the stemmed query
                for term in stemmed_query_set:
                    if term in term_dict:
                        t_f = 1 + math.log(stemmed_query.count(term), 10)
                        idf = math.log(1.0 * N / term_dict[term]['f'])
                        q_vec.append(t_f*idf)
                        # get the postings containing the term
                        cur_docs = get_postings(term)
                        # for each document, compute the normalized entry for the given term
                        # notice that the indexes for the term in both vectors are and need to be the same
                        for doc, tf in cur_docs:
                            norm = term_dict['DOC_NORM'][str(doc)]
                            t_f = 1 + math.log(tf, 10)
                            val = t_f/norm
                            if doc in doc_vects:
                                doc_vects[doc][term] = val
                            else:
                                doc_vects[doc] = {term: val}

                # Fill all document vectors with 0 for the query words they don't contain
                norm_doc_vects = dict()
                for word in stemmed_query_set:
                    if word in term_dict:
                        for doc, dic in doc_vects.iteritems():
                            if word in dic:
                                # if doc in the dict, append the next normalized value
                                if doc in norm_doc_vects:
                                    norm_doc_vects[doc].append(dic[word])
                                # if doc not in the dict of documents
                                else:
                                    norm_doc_vects[doc] = [dic[word]]
                            # if the word does not appear in the document
                            else:
                                if doc in norm_doc_vects:
                                    norm_doc_vects[doc].append(0)
                                else:
                                    norm_doc_vects[doc] = [0]

                q_vec_norm = math.sqrt(sum(i ** 2 for i in q_vec))
                q_vec_norm = [x / q_vec_norm for x in q_vec]
                res_vect = list()
                sorted_rel_docs = sorted(norm_doc_vects.keys())

                for doc in sorted_rel_docs:
                    doc_vec = norm_doc_vects[doc]
                    similarity = sum(map(mul, q_vec_norm, doc_vec))
                    # similarity = sum(map(mul, q_vec, doc_vec)) -> tried optimization
                    res_vect.append((doc, round(similarity,15)))
                # python sort method is stable and thus guarantee that docIDs with the same similarities
                # will remain in increasing order since they were inserted in the res_vect list in that order
                res_vect.sort(key=lambda x: x[1], reverse=True)
                final_restult = [i[0] for i in res_vect]
                # used to print the document ID together with the similarity
                '''if len(res_vect) > 10:
                    answer_to_queries.append(res_vect[:9])
                else:
                    answer_to_queries.append(res_vect)'''
                if len(final_restult) > 10:
                    answer_to_queries.append(final_restult[:10])
                else:
                    answer_to_queries.append(final_restult)
            else:
                answer_to_queries.append([])

    with open(file_of_output, 'w') as out:
        out_str = str()
        for l in answer_to_queries:
            out_str += ' '.join(str(el) for el in l) + '\n'
        out.write(out_str[:-1])     #removes the last '\n'




def compute_time(reps, func):
    """
    Test the execution speed of the search.
    Ref: https://stackoverflow.com/questions/1593019/is-there-any-simple-way-to-benchmark-python-script
    :param reps: Number of times the function should be executed
    :param func: The function to be executed
    :return: time needed to process all queries (in seconds)
    """
    from time import time
    start = time()
    for i in range(0, reps):
        func()
    end = time()
    return (end - start) / reps


if __name__ == "__main__":
    main()
    # Used for the execution time test
    # print compute_time(1, main)

